import math
import time
import networkx as nx

from traindsms.dsms.network import NetworkBaseClass
from missingadjunct.corpus import Corpus
from missingadjunct.utils import make_blank_sr_df
VERBOSE = False

class LON(NetworkBaseClass):
    """
    co-occurrence graph, generated by joining linear sentence sequence by shared words
    """

    def __init__(self, linear_corpus):
        NetworkBaseClass.__init__(self, linear_corpus)
        self.freq_threshold = 1
        self.co_mat = None
        self.network_type = 'Co-occurrence'


    def train(self):
        network_edge = []
        network_node = []
        count = 0
        epoch = 0
        start_time = time.time()

        for sentence in self.corpus:
            node_set = sentence
            edge_set = []
            l = len(sentence)
            for i in range(l-1):
                edge_set.append((node_set[i],node_set[i+1]))
            for node in node_set:
                if node not in self.word_dict:
                    self.word_dict[node] = len(self.word_dict)
                    self.freq_dict[node] = 1
                else:
                    self.freq_dict[node] = self.freq_dict[node] + 1
            network_edge.extend(edge_set)
            network_node.extend(node_set)
            count = count + 1
            if count >= 1000:
                count = 0
                epoch = epoch + 1000
                if VERBOSE:
                    print("{} sentences added to the linear graph.".format(epoch))
        end_time = time.time()
        time_joining_trees = end_time - start_time
        self.network = nx.Graph()
        network_node_set = set(network_node)
        self.node_list = list(network_node_set)
        network_edge_dict = {}
        for edge in network_edge:
            if edge in network_edge_dict:
                network_edge_dict[edge] = network_edge_dict[edge] + 1
            else:
                network_edge_dict[edge] = 1
        weighted_network_edge = []
        for edge in network_edge_dict:
            weighted_network_edge.append(edge + (math.log10(network_edge_dict[edge] + 1),))
        if VERBOSE:
            print()
            print('Weighted Edges:')
            for edge in weighted_network_edge:
                print(self.node_list.index(edge[0]), self.node_list.index(edge[1]), edge)
            print()
        self.network.add_weighted_edges_from(weighted_network_edge)
        if VERBOSE:
            print()
            print('{} used to join the trees.'.format(time_joining_trees))
            print()
        final_freq_dict = {}
        for word in self.freq_dict:
            if self.freq_dict[word] >= self.freq_threshold:
                final_freq_dict[word] = self.freq_dict[word]
        self.freq_dict = final_freq_dict
        if VERBOSE:
            print(len(final_freq_dict))
        self.word_list = [word for word in self.word_dict]
        self.get_adjacency_matrix()
        self.path_distance_dict = {node: {} for node in self.node_list}
        self.undirected_network = self.network.to_undirected()
        self.diameter = nx.algorithms.distance_measures.diameter(self.undirected_network)



def test_model():
    test_corpus = Corpus(include_location=False,
                         include_location_specific_agents=False,
                         num_epochs=1000,
                         seed=1,
                         )
    sentences = []
    for s in test_corpus.get_sentences():  # a sentence is a string
        tokens = s.split()
        sentences.append(tokens)

    dsm = LON(sentences)
    dsm.train()


    
    df_blank = make_blank_sr_df()
    df_results = df_blank.copy()
    instruments = df_blank.columns[3:]
    assert set(instruments).issubset(test_corpus.vocab)
    
    # fill in blank data frame with semantic-relatedness scores
    sr_bank = {}
    for verb_phrase, row in df_blank.iterrows():
        verb, theme = verb_phrase.split()
        scores = []
        if verb not in sr_bank:
            sr_bank[verb] = dsm.activation_spreading_analysis(verb, instruments, [])
        if theme not in sr_bank:
            sr_bank[theme] = dsm.activation_spreading_analysis(theme, instruments, [])

        for instrument in instruments:  # instrument columns start after the 3rd column
    
            sr = math.log(sr_bank[verb][instrument] * sr_bank[theme][instrument])
            scores.append(sr)
    
        # collect sr scores in new df
        df_results.loc[verb_phrase] = [row['verb-type'], row['theme-type'], row['phrase-type']] + scores
    print(df_results.loc['preserve pepper'].round(3))
    df_results.to_csv('df_test_lon.csv')

test_model()

# This is an attempt to get access to activation-spreading distance for all paths. It failed as number of possible paths
# could be very large basing on the graph structure, and time complexity could be exponential
'''''''''
def test_activation(): 
    test_corpus = Corpus(include_location=False,
                         include_location_specific_agents=False,
                         num_epochs=1,
                         seed=1,
                         )
    sentences = []
    for s in test_corpus.get_sentences():  # a sentence is a string
        tokens = s.split()
        sentences.append(tokens)

    dsm = LON(sentences)
    dsm.train()
    diameter = nx.algorithms.distance_measures.diameter(dsm.undirected_network)
    print(diameter)

    for node in dsm.node_list:
        start_time = time.time()
        dsm.get_path_distance(node,1,[])
        end_time = time.time()
        time_getting_paths = end_time - start_time
        print()
        print('{} used to get paths from {}'.format(time_getting_paths, node))
        print()
        #node_paths = dsm.path_distance_dict[node]
        #for target_node in node_paths:
            #print(node, target_node)
            #print(node_paths[target_node])



#test_activation()
'''
